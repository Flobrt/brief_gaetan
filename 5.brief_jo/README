# Projet: Pipeline ETL avec Airflow, Postgres et Streamlit 

Ce projet est un pipeline complet de tratiement de la données : 
 - Lecture d'un fichier CSV tous les 2ans (nouveaux jeaux = nouvelles données).
 - Chargement des données dans une base de données Postgres.
 - Requetage de la base depuis Streamlit.
 - Entierement conteneurisé avec Docker-compose. 


## Structure du projet 

5.brief_jo/   
├── app/                        ← Application streamlit
│   ├── app.py                  
│   ├── requirements.txt
│   └── Dockerfile
├── plugins/                    ← Dossier créé par Airflow
├── dags/                       ← Dossier dags
├── config/                     ← Dossier créé par Airflow
├── logs/                       ← Logs de airflow
├── data/                       ← Fichiers CSV pour simuler l'entrer de données
├── docker-compose.yml
├── preprocess_data.ipynb       ← Séparation des données de tests
└── README.md

## Lancer le projet

1. Cloner le repo

```
git clone <lien-du-repo>
cd project/
```

2. Lancer le projet 

```
docker compose up --build
```

3. Accèder à l'interface 

Streamlit App : http://localhost:8501

## Utilisation

### Ajout de données
Le fichier dags/data/fact_resultats_epreuves.csv va etre lu et les données vont être ajoutées en base si elles n'existent pas. 
Il faut ajouter de nouvelles données au fichier dags/data/fact_resultats_epreuves.csv pour simuler l'entrer de nouvelles données. 

### Lancer le DAG airflow :
1. Accèder à l'ui Airflow (http://localhost:8080)
2. Active le DAG csv_etl_pipeline
3. Il s'executera la prochaine fois dans 2ans (prochains jeux olympique)

###  Requêter les données
1. Accède à Streamlit (http://localhost:8501)
2. Tape une requête SQL, ex. :
```
SELECT * FROM table_jo LIMIT 10;
```
3. Clique sur Exécuter la requête
4. Télécharge les résultats en CSV si besoin



